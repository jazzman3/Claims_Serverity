{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claims Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrastructure\n",
    "- VSH: Amazon EC2\n",
    "- Availability zone: us-east-1d\n",
    "- Instance type: C3\n",
    "- Application: Compute Optimized\n",
    "- Model: c3.2xlarge\n",
    "- vCPU: 8\n",
    "- Mem (GiB): 15\n",
    "- SSD Storage (GB): 2x80\n",
    "- Operational system: Windows Server 2012 R2\n",
    "- Platform: Anaconda2 64 bits\n",
    "- Environment: Jupyter Notebook\n",
    "\n",
    "### Features:\n",
    "- High Frequency Intel Xeon E5-2680 v2 (Ivy Bridge) Processors\n",
    "- Support for Enhanced Networking\n",
    "- Support for clustering\n",
    "- SSD-backed instance storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rjazmin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# Transform and scale\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn import linear_model\n",
    "from sklearn import ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-0890007ab2fd>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-0890007ab2fd>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    train = pd.read_csv(???, index_col=???)\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### CODING TIME 1: LOAD THE DATA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "### TODO: Load train and test dataframes from CSV files.\n",
    "### Define the filepath and the index column.\n",
    "### Call the variables 'train' and 'test'.\n",
    "train = pd.read_csv(???, index_col=???)\n",
    "test = pd.read_csv(???, index_col=???)\n",
    "\n",
    "end = time()\n",
    "print \"Load CSVs in {:.1f} seconds.\\n\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Data Shape\n",
    "print train.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Columns\n",
    "print list(train.columns)\n",
    "print \"\"\n",
    "print list(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 1: LOAD THE DATA\n",
    "\n",
    "# Define variables\n",
    "loss = train[???]\n",
    "\n",
    "### TODO: After defining the loss variable as the loss column of the dataframe, define the features variable.\n",
    "### Call the new variable 'features'.\n",
    "features = train.drop(???, ???)\n",
    "\n",
    "### TODO: Concatenate the training and test set to the same DataFrame.\n",
    "### Call the new variable 'train_test'\n",
    "train_test = pd.concat((???, ???))\n",
    "\n",
    "numeric_features = list(train_test.dtypes[train_test.dtypes != \"object\"].index)\n",
    "categorical_features = list(features.drop(numeric_features, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Data\n",
    "print \"Train_Test Data Set Describe\"\n",
    "print train_test.describe()\n",
    "print \"\\nLoss Describe\"\n",
    "print loss.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 2: UNDERSTAND THE DATA\n",
    "\n",
    "# Print Skew\n",
    "\n",
    "### TODO: Show the skew of each numerical feature for the train_test set and the loss.\n",
    "### Tip: The skew method will ignore categorical features, if present\n",
    "print \"Train_Test Data Set Skew\"\n",
    "print train_test.???()\n",
    "print \"\\nLoss Skew\"\n",
    "print loss.???()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 2: UNDERSTAND THE DATA\n",
    "\n",
    "# Numerical Features Visualization\n",
    "\n",
    "# Import plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a dataframe with only numerical features\n",
    "data = train_test[numeric_features] \n",
    "\n",
    "### TODO: Get the names of the columns\n",
    "### Call the variable 'cols'\n",
    "cols = data.??? \n",
    "\n",
    "# Plot violin for all attributes in a 7x2 grid\n",
    "n_cols = 2\n",
    "n_rows = 7\n",
    "\n",
    "### TODO: Show violin plots for all numerical features\n",
    "for i in range(n_rows):\n",
    "    fg,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(12, 8))\n",
    "    for j in range(n_cols):\n",
    "        sns.violinplot(y=cols[i*n_cols+j], data=data, ax=ax[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 3: UNDERSTAND THE DATA (continue)\n",
    "\n",
    "# Categorical Features Visualization\n",
    "\n",
    "data = train_test[categorical_features]\n",
    "cols = data.columns\n",
    "\n",
    "# Count plot for all categorical features in a 29x4 grid\n",
    "n_cols = 4\n",
    "n_rows = 29\n",
    "\n",
    "### TODO: Show count plots for all categorical features\n",
    "for i in range(n_rows):\n",
    "    fg,ax = plt.subplots(nrows=1,ncols=n_cols,sharey=True,figsize=(12, 8))\n",
    "    for j in range(n_cols):\n",
    "        sns.countplot(x=cols[i*n_cols+j], data=data, ax=ax[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 4: THE TARGET VARIABLE\n",
    "\n",
    "### TODO: Show violin plots for the target variable\n",
    "\n",
    "# Loss Visualization\n",
    "sns.violinplot(data=train,y=\"???\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 5: PRE-PROCESSING THE DATA\n",
    "\n",
    "# Transform data\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "### TODO: Apply the boxcox function for all numerical features\n",
    "### which are skewed more than 0.25 or less than -0.25\n",
    "def aply_boxcox(df, cols, fact):\n",
    "    skewed_feats = df[cols].apply(lambda x: ???(???))\n",
    "    skewed_feats = skewed_feats[abs(???) > fact].index\n",
    "    for feat in skewed_feats:\n",
    "        df[feat], lam = ???(df[feat]+1)\n",
    "\n",
    "### TODO: Apply pandas.factorize to all categorical features\n",
    "### You may want to improve it in the future to make it one hot encoding\n",
    "def factorize_features(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = pd.???(???, sort=???)[???]\n",
    "\n",
    "start = time()\n",
    "\n",
    "aply_boxcox(train_test, numeric_features, 0.25)\n",
    "loss, loss_lam = boxcox(loss)\n",
    "factorize_features(train_test, categorical_features)\n",
    "train_rows = train_test.loc[features.index]\n",
    "test_rows = train_test.loc[test.index]\n",
    "\n",
    "end = time()\n",
    "print \"Transform data in {:.1f} seconds.\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_test.cat100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 6: SCALING THE DATA\n",
    "\n",
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### TODO: Fit the train_test data to the scaler\n",
    "### TODO: Transform train and test data sets using the scaler parameter\n",
    "### TODO: Also scale the loss variable\n",
    "\n",
    "def scale_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = ???()\n",
    "        scaler.???(X)\n",
    "    X = scaler.???(X)\n",
    "    return X, scaler\n",
    "\n",
    "start = time()\n",
    "\n",
    "_, scaler = scale_data(train_test)\n",
    "train, _ = scale_data(train_rows, scaler)\n",
    "test, _ = scale_data(test_rows, scaler)\n",
    "y, scaler = scale_data(loss.reshape(-1, 1))\n",
    "\n",
    "end = time()\n",
    "print \"Scale data in {:.1f} seconds.\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 7: DEFINING THE PARAMETERS\n",
    "\n",
    "# Define Grid Search Function\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "### TODO: Create a r2 score to be used in the grid search.\n",
    "### TODO: use ShuffleSplit object to define the cross validation method the Grid Search\n",
    "### TODO: Create the Grid Search object and return it\n",
    "\n",
    "def fit_model(X, y, estim, params):\n",
    "    score = make_scorer(???, greater_is_better=???)\n",
    "    sss = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 86401)\n",
    "    grid = GridSearchCV(estimator=???, param_grid=???, scoring=???, cv=???, n_jobs=6)\n",
    "    grid = grid.???(X, y)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 8: DIFFERENT ML ALGORITHMS\n",
    "### TODO: Fit Bayesian Ridge\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "start = time()\n",
    "\n",
    "bay = BayesianRidge()\n",
    "param_bay = {'n_iter':[2, 4, 8]}\n",
    "bay_grid = fit_model(???, ???, ???, ???)\n",
    "\n",
    "end = time()\n",
    "print \"BAY grid search in {:.1f} seconds.\".format(end - start)\n",
    "print \"BAY best score:  {:.4f}\".format(bay_grid.best_score_)\n",
    "print bay_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODING TIME 8: DIFFERENT ML ALGORITHMS\n",
    "### TODO: Fit GradientBoostingRegressor\n",
    "\n",
    "start = time()\n",
    "\n",
    "gbr = ensemble.GradientBoostingRegressor()\n",
    "# param_gbr = {'n_estimators' : [25,50, 200], 'max_depth' : [6, 8, 10, 12], 'random_state' : [864]\n",
    "param_gbr = {'n_estimators' : [25], 'max_depth' : [10], 'random_state' : [864]}\n",
    "gbr_grid = fit_model(???, ???, ???, ???)\n",
    "\n",
    "end = time()\n",
    "print \"GBR grid search in {:.1f} seconds.\".format(end - start)\n",
    "print \"GBR best score:  {:.4f}\".format(gbr_grid.best_score_)\n",
    "print gbr_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "y_gbr = gbr_grid.best_estimator_.predict(test)\n",
    "y_gbr = scaler.inverse_transform(y_gbr)\n",
    "y_gbr = np.exp(np.log(loss_lam*y_gbr+1)/loss_lam)\n",
    "\n",
    "end = time()\n",
    "print \"GBR predict in {:.1f} seconds.\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = pd.read_csv('data/test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df.insert(0, 'id', list(test_raw.index))\n",
    "df.insert(1, 'loss', list(y_gbr))\n",
    "df.to_csv('data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss Predicted Visualization\n",
    "\n",
    "sns.violinplot(data=y_gbr)\n",
    "plt.show()\n",
    "print pd.DataFrame({'predicted': y_gbr}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
